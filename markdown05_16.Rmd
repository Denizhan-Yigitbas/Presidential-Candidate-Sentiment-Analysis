---
title: "Using Sentiment Analysis to Explore the Twitter Feeds of Presidential Candidates"
author: "Connor Rothschild and Denizhan Yigitbas"
date: "5/16/2019"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      out.width='1000px', out.height='600px', dpi=200)
```

The R code for this post can be found [here](.
## Research Question and Goal

How do presidential candidates differ in their use of Twitter to publicize policy positions, garner support, and participate in the political arena? Are certain types of Tweets more effective at attracting support, as evidenced by a greater number of Tweet interactions? 

One way to explore these questions is by using *sentiment analysis*, or the process of computationally categorizing opinions and emotionality in a piece of text. Some data scientists, for example, have used this technique to argue that Donald Trump's Twitter timeline is populated by [different Tweets from different people](http://varianceexplained.org/r/trump-tweets/). 

Sentiment analysis has yet to be utilized to explore how *different* candidates use Twitter in different ways. By utilizing [sentence-level sentiment analysis](https://medium.com/@ODSC/an-introduction-to-sentence-level-sentiment-analysis-with-sentimentr-ac556bd7f75a) on individual Tweets as well as word-by-word analysis using the [NRC Word-Emotion Association Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), we're able to explore this exact question. The analysis leverages the power of the [sentimentr package](https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf) to better understand what types of emotions "sell" on Twitter and how candidates use the platform as a mechanism to attract political support. 

## Load Libraries

```{r load libraries}
library(dplyr)
library(purrr)
library(twitteR)
library(tidyr)
library(lubridate)
library(scales)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(readr)
library(sentimentr)
library(dplyr)
library(syuzhet)
library(broom)
library(cleanNLP)
library(textclean)
library(ggrepel)
library(topicmodels)
library(tm)
library(stm)
library(quanteda)
```

## Data Collection

We begin our analysis with the data collection process. We use Python to gather a massive amount of Tweets (37,218!) from 12 presidential candidates' Twitter feeds. Specifically, we used two different methods for pulling Tweets. The main method involved using Twitter’s Tweepy Python library. We created an API that allowed us to pull various information about the most recent 3200 tweets by a user. [The API link could be found here](https://github.com/Denizhan-Yigitbas/Presidential-Candidate-Sentiment-Analysis/blob/master/DenizhanCode/TwitterUser.py).

In a later step, we will go further back in time to pull Tweets from 2016 presidential candidates for a comparative analysis. In order to do this, we use a [public GitHub repository](https://github.com/bpb27/twitter_scraping) that creates a scraper to pull candidate Tweets. The scraper automatically scrolls through the inputed date range on a given candidate's website and mines data while scrolling.

You can find further documentation of our data pulling process on [our Github](https://github.com/Denizhan-Yigitbas/Presidential-Candidate-Sentiment-Analysis). 

## Cleaning Data

The data we pull contains the information we need but is not quite ready for analysis.

```{r read data}
uncleaned <- read_csv("customTwitterUsersCombinedUncleaned.csv")
head(uncleaned$text)
```

Specifically, the presence of links and images may cause some problems as they are high in frequency but provide no insights into Tweet sentiment. Moreover, Tweets often include a large number of Twitter-specific characters such as @s and #s. 

We can clean this type of text using some RegEx and the [stringr package](https://cran.r-project.org/web/packages/stringr/stringr.pdf). Then, using the *tweetBinary* column in our dataset, we filter out Retweets (because we are interested in candidates' sentiment, not their followers').

```{r clean data}
cleaned <- uncleaned %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>% 
  mutate(text = str_replace_all(text, "@[A-Za-z0-9_]+[A-Za-z0-9-_]+", "")) %>% 
  mutate(text = str_replace_all(text, "@", "")) %>% 
  mutate(text = str_replace_all(text, "—", " ")) %>% 
  mutate(text = str_replace_all(text, "http\\S+", "")) %>%
  mutate(createdDate = parse_date_time(createdDate, 'mdy')) %>%  
  filter(!is.na(text)) %>% 
  filter(text!="")

cleaned$candidate <- recode(cleaned$candidate, "amyklobuchar"="Amy Klobuchar")
cleaned$candidate <- recode(cleaned$candidate, "ewarren"="Elizabeth Warren")
cleaned$candidate <- recode(cleaned$candidate, "BetoORourke"="Beto O'Rourke")
cleaned$candidate <- recode(cleaned$candidate, "JoeBiden"="Joe Biden")
cleaned$candidate <- recode(cleaned$candidate, "JulianCastro"="Julian Castro")
cleaned$candidate <- recode(cleaned$candidate, "TulsiGabbard"="Tulsi Gabbard")
cleaned$candidate <- recode(cleaned$candidate, "PeteButtigieg"="Pete Buttigieg")
cleaned$candidate <- recode(cleaned$candidate, "KamalaHarris"="Kamala Harris")
cleaned$candidate <- recode(cleaned$candidate, "BernieSanders"="Bernie Sanders")
cleaned$candidate <- recode(cleaned$candidate, "CoryBooker"="Cory Booker")
cleaned$candidate <- recode(cleaned$candidate, "AndrewYang"="Andrew Yang")
cleaned$candidate <- recode(cleaned$candidate, "SenGillibrand"="Kirsten Gillibrand")

cleanedtweets <- cleaned %>% 
  filter(tweetBinary == 1)

cleanedretweets <- cleaned %>% 
  filter(tweetBinary == 0)
```

This leaves us with 28,347 cleaned Tweets:

```{r preview, echo=FALSE}
cleanedtweets
```
## Preliminary Analysis

We begin with a preliminary look at the structure of our data. First, how equal is the distribution of data among candidates?

```{r overview}
cleanedtweets %>% 
  group_by(candidate) %>% 
  distinct(id, .keep_all = TRUE) %>% 
  ggplot(aes(x=forcats::fct_infreq(candidate))) +
  geom_bar(stat="count") +
  coord_flip() +
  labs(x="Number of Tweets",
       y="Candidate")
```

Our dataset is lacking in Tweets from Julian Castro and Joe Biden, and to some extent Cory Booker. This is mostly because those candidates don't Tweet as much; [Joe Biden's Twitter account](https://twitter.com/JoeBiden) only has 1,700 Tweets. But it's also because some candidates Retweet more than others, and we filter those out. 

Next, how does the distribution of Tweets over time look? 

```{r tweets over time}
cleanedtweets %>% 
  ggplot(aes(x=month(createdDate))) +
  geom_histogram(position = "identity", bins = 12, show.legend = FALSE) +
  labs(x="Month of Creation",
       y="Number of Tweets") +
  scale_x_continuous(name = "Month", breaks=c(3,6,9,12), labels=c("Mar.", "June","Sept.", "Dec."))

cleanedtweets %>% 
  ggplot(aes(x = month(createdDate), fill = candidate)) +
  geom_histogram(position = "identity", bins = 12, show.legend = FALSE) +
  facet_wrap(~candidate, scales = "free") +
  labs(x="Month of Creation",
       y="Number of Tweets") +
  scale_x_continuous(name = "Month", breaks=c(3,6,9,12), labels=c("Mar.", "June","Sept.", "Dec.")) +
  theme(axis.text.x = element_text(size = 6))
```

From this, its obvious that Andrew Yang Tweets *way more* than everyone else, given that most of his tweets (>2000) are from the last three months (February, March, and April).

## Sentiment Analysis Using sentimentr
Following this preliminary look at our data, we can begin with what we're really interested in: sentiment analysis of candidate Tweets. 

We initially performed our analysis using the [syuzhet package](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html) in R, but found its output to be imprecise and misleading. This was because the syuzhet library was specifically made for sentiment analysis of fiction text (and the [domain-specifity of sentiment analyses is very important](https://sentic.net/wisdom2017labille.pdf)). We also ran into problems because the syuzhet library failed to account for [valence shifters](https://www.aaai.org/Papers/Symposia/Spring/2004/SS-04-07/SS04-07-020.pdf) (such as "not", "very", or "doesn't"). 

The sentimentr package addresses both of these concerns; it is fine-tuned for sentence-level analysis, [has been used on Tweets in prior analyses](https://blog.exploratory.io/twitter-sentiment-analysis-scoring-by-sentence-b4d455de3560), and it accounts for [contextual valence shifters](https://www.aaai.org/Papers/Symposia/Spring/2004/SS-04-07/SS04-07-020.pdf). 

As an example, the syuzhet package would code the sentence "I don't hate ice cream" as relatively negative, because the keyword *hate* determines the overall polarity of the sentence. The sentimentr package accounts for the negator *don't* and reverses the polarity score to be slightly positive. For a more thorough review of sentimentr, see [this post](https://cran.r-project.org/web/packages/sentimentr/readme/README.html) from its creator.
 
Let's add sentiment to the dataset:
``` {r add sentiment}
tweets <- cleanedtweets %>%
  sentimentr::get_sentences() %>% 
  sentimentr::sentiment(polarity_dt = lexicon::hash_sentiment_jockers_rinker,
                        valence_shifters_dt = lexicon::hash_valence_shifters) %>% 
  group_by(element_id) %>% 
  mutate(text = add_comma_space(text)) %>% 
  mutate(tweet = paste(text, collapse=" ")) %>% 
  ungroup() %>% 
  mutate(mentiontrump = ifelse(str_detect(tweet, "Trump")==TRUE,yes=1,no=0)) %>% 
  mutate(text = replace_white(text)) %>% 
  mutate(tweet = replace_white(tweet)) %>% 
  group_by(tweet) %>% 
  mutate(tweetlevelsentiment = mean(sentiment)) %>% 
  ungroup %>% 
  filter(!is.na(word_count)) %>% 
  filter(text!=" ")  %>% 
  mutate(sentimentbinary = ifelse(sentiment>0,"Positive", 
                            ifelse(sentiment<0,"Negative","Neutral"))) %>% 
  mutate(tweetsentimentbinary = ifelse(tweetlevelsentiment>0,"Positive",
                                ifelse(tweetlevelsentiment<0,"Negative","Neutral"))) %>% 
  mutate(distancefromzero = abs(sentiment)) %>% 
  select(candidate, id, tweet, text, sentiment, sentimentbinary, 
         tweetlevelsentiment, tweetsentimentbinary, distancefromzero,
         favoriteCount, retweetCount, createdDate, mentiontrump,
         element_id, sentence_id, word_count)

summary(tweets)
```
The above code creates a few variables to determine sentiment:

1. Unnests each Tweet into its respective sentences (*text*)
2. Provides a sentiment score for each sentence (*sentiment*)
3. Concatenates the sentences back into their original Tweet form (*tweet*)
4. Provides a Tweet-level sentiment (*tweetlevelsentiment*) score by averaging each sentence's score for a Tweet
5. Creates a three-part classification of Tweets as neutral, positive, or negative, on both the sentence- and Tweet-level
6. Creates a *distancefromzero* variable which captures emotionality in any direction

## Sentence- and Tweet-Level Sentiment Analysis

First, we can explore overall sentiment by depicting what proportion of sentences fall into the categories of *negative* (sentiment < 0), *positive* (sentiment > 0), or *neutral* (sentiment = 0).
```{r sentiment overview}
tweets %>% 
  ggplot(aes(x=sentimentbinary, fill=sentimentbinary)) +
  geom_bar(stat="count", show.legend = FALSE) +
  labs(x="Sentiment",
       y="Number of Sentences",
       title = "Frequency of Sentiment by Sentence")
```

Individual sentences tend to be overwhelmingly positive. Does a different trend arise when we look at entire Tweets (by averaging the scores of each individual sentence)?

``` {r sentence sentiment}
tweets %>% 
  distinct(id, .keep_all = TRUE) %>% 
  ggplot(aes(x=tweetsentimentbinary, fill=tweetsentimentbinary)) +
  geom_bar(stat="count", show.legend = FALSE) +
  labs(x="Sentiment",
       y="Number of Tweets",
       title = "Frequency of Sentiment by Tweet")
```

This reveals that there is less neutrality in Tweets than there is in sentences. This makes sense when we consider the likelihood of a Tweet being entirely composed of neutral sentences (sentiment = 0) is relatively low.

An additional question is if sentiment differs by candidate.

``` {r sentiment by candidate}
tweets %>% 
  distinct(id, .keep_all = TRUE) %>% 
  group_by(candidate, tweetsentimentbinary) %>% 
  summarise(n = n()) %>% 
  mutate(percentsentiment = 100*(n/sum(n))) %>% 
  ggplot(aes(x=tweetsentimentbinary, y= percentsentiment, fill=tweetsentimentbinary)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~ candidate) +
  labs(x="Sentiment",
       y="Number of Tweets",
       title = "Frequency of Sentiment by Tweet")
```

Evidently, Kamala Harris, Kirsten Gillibrand, and Bernie Sanders are almost *never* neutral. By contrast, Joe Biden, Andrew Yang, and Beto O'Rourke tend to be neutral more often than most candidates.

Aside from the binary classification of positive/negative, we may also be curious which candidates tend to be the *most* positive and the *most* negative:

``` {r nonbinary sentiment}
tweets %>%
  group_by(candidate) %>% 
  summarise(sentiment = mean(sentiment)) %>% 
  arrange(desc(sentiment)) %>% 
  ggplot(aes(x = reorder(candidate, sentiment), y = sentiment, fill=candidate)) +
  geom_col(show.legend = FALSE) +
  xlab("Candidate") +
  ylab("Sentiment") +
  ggtitle("Average Sentiment by Candidate", subtitle = "With higher scores indicating more positive content")  +
  coord_flip()
```

There is considerable variance in positivity across candidates. Pete Buttigieg's Tweets tend to be, on average, 3.74x more positive than those from Kamala Harris.

## Incorporating the NRC Emotion Lexicon

After exploring how sentiment differs by candidate, we can perform additional analyses by looking at the *individual words a candidate uses*. The inspiration for this decision was the book *[Text Mining with R](https://www.tidytextmining.com/sentiment.html)*, written by [Julia Silge](https://juliasilge.com/) and [David Robinson](http://varianceexplained.org/). 

We use the [NRC Word-Emotion Association Lexicon](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), which is a crowdsourced list of 14,182 words and their most frequent association with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust). This allows us to probe further into candidate Twitter feeds to see how candidates differ in their use of specific emotions.

We create the *tweet_words* dataset with the *[unnest_tokens](https://www.rdocumentation.org/packages/tidytext/versions/0.2.0/topics/unnest_tokens)* function found in the *[tidytext package](https://www.rdocumentation.org/packages/tidytext)*. We then inner join the dataset to the NRC lexicon under the name *tweet_words_w_nrc*.

``` {r create tweet_words}
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
tweet_words <- tweets %>% 
  unnest_tokens(word, text, token = "regex", pattern = reg, drop = FALSE) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)
colnames(nrc)[colnames(nrc)=="sentiment"] <- "wordsentiment"

tweet_words_w_nrc <- tweet_words %>%
  inner_join(nrc, by = "word")
```

One interesting question is which words garner the greatest Tweet attention, as evidenced by Favorites and Retweets. We answer this question by finding the median number of interactions (Retweets + Favorites) for non-stop-words (e.g. "the", "of", and "are") and plotting the most popular. 

``` {r words by tweet interactions}

word_by_interactions <- tweet_words %>% 
  group_by(id, word, candidate) %>% 
  mutate(interactions = favoriteCount+retweetCount) %>% 
  summarise(interactions = first(interactions)) %>% 
  group_by(candidate, word) %>% 
  summarise(interactions = median(interactions), uses = n()) %>%
  filter(interactions != 0) %>%
  ungroup()

word_by_interactions %>%
  filter(uses > 5) %>%
  group_by(candidate) %>%
  top_n(10, interactions) %>%
  arrange(interactions) %>%
  ungroup() %>%
  mutate(word = factor(word, unique(word))) %>%
  ungroup() %>%
  ggplot(aes(word, interactions, fill = candidate)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ candidate, scales = "free") +
  coord_flip() +
  labs(x = NULL, 
       y = "Median # of Interactions for Tweets containing each word",
       title = "Most Popular Words Used on Twitter") +
  theme(axis.text.x = element_text(size = 6, angle = 20),
        axis.text.y = element_text(size = 5),
        strip.text.x = element_text(size = 8))
```

The plot reveals some interesting insights about what topics are especially popular for certain candidates. It seems as if Tulsi Gabbard performs best when she mentions issues related to international security (*terrorism, waging, jihadists, genocidal*). Cory Booker received the most Retweets and Favorites during the Kavanaugh hearings in September 2018. Joe Biden is an especially interesting case; nearly all of his most popular words come from one phrase: "a battle for the soul of this nation." He uses that phrase---or some variation of it---[a lot](https://twitter.com/search?q=soul%20of%20this%20nation%20from%3AJoeBiden&src=typd).

More generally, words tangentially related to some legal issue in the Trump administration tend to perform the best on Twitter:

``` {r word interactions unfaceted}
word_by_interactions %>%
  filter(uses > 5) %>%
  top_n(10, interactions) %>%
  arrange(interactions) %>%
  ungroup() %>%
  mutate(word = factor(word, unique(word))) %>%
  ungroup() %>%
  ggplot(aes(word, interactions)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(x = NULL, 
       y = "Median # of Interactions for Tweets containing each word",
       title = "Most Popular Words Used on Twitter")
```

By incorporating the NRC lexicon, we can see if any specific emotions "sell." In other words, we can see if some emotions outperform others when it comes to getting Tweet attention.

``` {r emotion by interactions}
candidate_interactions_by_sentiment <- tweet_words_w_nrc %>%
  group_by(id, wordsentiment, candidate) %>% 
  mutate(interactions = favoriteCount+retweetCount) %>% 
  summarise(interactions = first(interactions)) %>% 
  group_by(candidate, wordsentiment) %>% 
  summarise(interactions = mean(interactions), uses = n()) %>% 
  ungroup() 

candidate_interactions_by_sentiment %>% 
  group_by(candidate) %>% 
  ggplot(aes(x=reorder(wordsentiment, interactions), y=interactions, fill=wordsentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ candidate, scales = "free_x") +
  coord_flip() +
  ylab("Average # of Interactions") +
  xlab(element_blank()) +
  ggtitle("Popularity of Tweet Sentiments") +
  theme(axis.text.x = element_text(size = 6, angle = 20),
        axis.text.y = element_text(size = 6))

candidate_interactions_by_sentiment %>% 
  group_by(wordsentiment) %>% 
  summarise(meaninteractions = mean(interactions)) %>% 
  ggplot(aes(x=reorder(wordsentiment, meaninteractions), y=meaninteractions, fill=wordsentiment)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  ylab("Average # of Interactions") +
  xlab(element_blank()) +
  ggtitle("Popularity of Tweet Sentiments")
```

The results from this are really consistent: negative emotions---like disgust, anger, sadness, and fear---sell.

Even when we stop looking at specific emotions and turn back to our initial sentiment scale of -2 to 2, we find the trend is consistent and statistically significant. 

``` {r polarity vs popularity}
tweets %>%
  distinct(id, .keep_all = TRUE) %>% 
  mutate(interactions = favoriteCount + retweetCount) %>% 
  filter(interactions < 90000) %>% 
  filter(sentiment<1.5) %>% 
  ggplot(aes(x=tweetlevelsentiment, y=interactions)) +
  geom_point(alpha = .05) +
  geom_smooth(method=lm, se=TRUE) +
  ylab("Number of Interactions") +
  xlab("Sentiment Score") +
  ggtitle("Relationship Between Tweet Sentiment and Interactions")
```

**A linear regression of Tweet interactions by sentiment reveals that a one unit increase in sentiment results in a loss of 4492 Tweet interactions (*p* < 0.0001).**

## The Trump Analysis

A final topic of interest is how Donald Trump affects the sentiment of candidate Tweets. Knowing that Donald Trump is a unique president with uniquely low levels of popularity, one may hypothesize that Tweets focusing on or mentioning the president may be different from most others. 

Earlier, we created the variable *mentiontrump* using the *str_detect* function from the [stringr package](https://www.rdocumentation.org/packages/stringr/versions/1.4.0). By analyzing differences between Tweets which mention Trump and those that don't, we can understand how the "Trump variable" impacts Tweet behavior and popularity.

First, who talks about Trump? Are some candidates (e.g. those who need to access Trump's base in order to secure the Democratic nomination) less likely to attack the president than others?

```{r mentiontrump candidate frequency}
tweets %>% 
  distinct(id, .keep_all = TRUE) %>% 
  group_by(candidate) %>% 
  summarise(mentiontrump = sum(mentiontrump==1), 
            totaltweets = n(), 
            percent = 100*(mentiontrump/totaltweets)) %>% 
  ggplot(aes(x=reorder(candidate, percent), y=percent, fill = candidate)) +
  geom_col(show.legend = FALSE) + 
  xlab(element_blank()) +
  ylab("Percent of Tweets Mentioning Trump") +
  ggtitle("Candidate Frequency of Mentioning Trump") +
  coord_flip()
```

Joe Biden, who pitches himself as a moderate and will likely see electoral success by engaging "Never Trump" voters and moderate conservatives, mentions Trump much less frequently than more "radical" candidates like Bernie Sanders. 

Next, how does sentiment change in Tweets that mention Trump compared to those that don't?

```{r mentiontrump sentiment}
tweets %>% 
  group_by(mentiontrump) %>% 
  summarise(meansentiment = mean(sentiment)) %>% 
  ggplot(aes(x=mentiontrump, y=meansentiment, fill=mentiontrump)) +
  geom_col(show.legend = FALSE) +
  ggtitle("Average Sentiment", subtitle = "Tweets that mention Trump vs those that don't") +
  ylab("Sentiment") +
  xlab(element_blank()) +
  scale_x_continuous(breaks=c(0,1), labels = c("Does Not Mention", "Does Mention"))
```

Unsurprisingly, Tweets about Donald Trump are significantly more negative than those about other topics.

Again, our word-by-word analysis allows us to get more fine-tuned than this simple binary classification:

```{r mentiontrump specific emotion}
tweet_words_w_nrc %>% 
  mutate(mentiontrump = as.factor(mentiontrump)) %>% 
  group_by(mentiontrump, wordsentiment) %>% 
  summarise(n = n()) %>% 
  mutate(percentsentiment = 100*(n/sum(n))) %>% 
  ggplot(aes(group=mentiontrump, x=reorder(wordsentiment,percentsentiment), 
             fill=mentiontrump, y=percentsentiment)) +
  geom_bar(position=position_dodge(), stat="identity") +
  scale_fill_discrete(name="Mentions Trump",
                    breaks=c(0,1),
                    labels=c("No", "Yes")) +
  labs(x=element_blank(), 
       y = "Percent of Words", 
       title = "Sentiment as a Proportion of Overall Tweets", 
       subtitle = "Comparing Tweets mentioning Trump with those that don't")
```

The trend holds true: positive emotions like joy, trust, and anticipation are less common in Tweets that mention Donald Trump. The corrolary is also true. Negative emotions such as disgust, sadness, anger, and fear are more prevalent in Tweets that mention the president.

Aside from emotion, does the "Trump variable" affect Tweet popularity? One may hypothesize that Trump's [historically low popularity](https://projects.fivethirtyeight.com/trump-approval-ratings/) may lend itself to higher popularity in Tweets which disparage him.

``` {r mentiontrump popularity}
tweets %>% 
  distinct(id, .keep_all = TRUE) %>% 
  mutate(interactions = retweetCount+favoriteCount) %>% 
  mutate(mentiontrump = as.factor(mentiontrump)) %>% 
  group_by(mentiontrump) %>% 
  summarise(interactions = mean(interactions)) %>% 
  ggplot(aes(x=mentiontrump, y=interactions, fill=mentiontrump)) +
  geom_col(show.legend = FALSE) +
  scale_x_discrete(breaks=c(0,1), labels = c("Does Not Mention", "Does Mention")) +
  labs(x="Mentions Trump",
       y = "Average # of Interactions", 
       title = "Average Tweet Popularity", 
       subtitle = "Comparing Tweets mentioning Trump with those that don't") 
```

Tweets which mention the president tend to get 2.2x the number of interactions (13,087) than those which don't (5912).

## Next Steps

We want to continue this work by expanding upon current analyses of 2020 presidential candidates. We are curious about how other variables (such as position in race) affect the use of certain emotions and sentiment, or what kind of differences arise when comparing candidates of different demographic backgrounds.

Finally, we would like to see how Twitter has *evolved* as a platform by using 2016 presidential candidates' Twitter feeds as a comparison. We are curious if Tweet content has changed in a way which reflects general trends in politics, such as growing partisanship, heightened polarization, and more identity-centric notions of political identity.